{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv dataset then edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"googleplaystore_user_reviews.csv\", na_values=\"nan\")\n",
    "df = df.dropna(subset=['App','Translated_Review','Sentiment'], how ='any')\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Positive'],'1')\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Negative'],'0')\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Neutral'],'1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reviews are categorized as lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37427\n"
     ]
    }
   ],
   "source": [
    "review_lines = list()\n",
    "lines = df['Translated_Review'].values.tolist()\n",
    "print (len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization and removing punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37427"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in lines :\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table =str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)\n",
    "len(review_lines)\n",
    "#print(review_lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\New folder\\envs\\awesome\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word: 21481\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=review_lines,size=100,window = 5,workers =4,min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "print('total word: %d' %len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\New folder\\envs\\awesome\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "filename = 'r.txt'\n",
    "model.wv.save_word2vec_format(filename,binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding as a directory of words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('','r.txt'),encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting the word embedding into tokenized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 21481 unique tokens \n",
      "Shape of review  (37427, 100)\n",
      "shape of senti (37427,)\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(review_lines)\n",
    "sequences = tk.texts_to_sequences(review_lines)\n",
    "word_index = tk.word_index\n",
    "print(\"found %s unique tokens \" % len(word_index))\n",
    "review_pad = pad_sequences(sequences,maxlen=100)\n",
    "sentiment = df['Sentiment'].values\n",
    "print('Shape of review ', review_pad.shape)\n",
    "print('shape of senti' , sentiment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map embeddings from the loaded word2vec model for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21482\n"
     ]
    }
   ],
   "source": [
    " num_words = len(word_index) + 1\n",
    "embedd = np.zeros((num_words,100))\n",
    "\n",
    "for word , i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedd_vec = embeddings_index.get(word)\n",
    "    if embedd_vec is not None:\n",
    "        embedd[i] = embedd_vec     \n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 64 \n",
    "num_epochs = 10\n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding matrix as input to the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 100)          2148200   \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 100, 64)           44864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 50, 64)            28736     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 25, 64)            28736     \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 25, 64)            28736     \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 25, 64)            28736     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 2,308,777\n",
      "Trainable params: 160,577\n",
      "Non-trainable params: 2,148,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim,\n",
    "          weights=[embedd], input_length=100, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='sigmoid', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='sigmoid', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='sigmoid', padding='same'))\n",
    "model.add(Conv1D(num_filters, 7, activation='sigmoid', padding='same'))\n",
    "model.add(Conv1D(num_filters, 7, activation='sigmoid', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the sentiment classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_pad  (29942, 100)\n",
      "shape of y_train  (29942,)\n",
      "shape of X_test_pad  (7485, 100)\n",
      "shape of y_train  (7485,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation = int (VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation]\n",
    "y_train = sentiment[:-num_validation]\n",
    "X_test_pad = review_pad[-num_validation:]\n",
    "y_test = sentiment[-num_validation:]\n",
    "\n",
    "\n",
    "\n",
    "print('shape of X_train_pad ', X_train_pad.shape)\n",
    "print('shape of y_train ', y_train.shape)\n",
    "\n",
    "print('shape of X_test_pad ', X_test_pad.shape)\n",
    "print('shape of y_train ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the classification model on train and validation test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29942 samples, validate on 7485 samples\n",
      "Epoch 1/10\n",
      " - 36s - loss: 0.1657 - acc: 0.7771 - val_loss: 0.1454 - val_acc: 0.7848\n",
      "Epoch 2/10\n",
      " - 34s - loss: 0.1384 - acc: 0.7952 - val_loss: 0.1251 - val_acc: 0.8138\n",
      "Epoch 3/10\n",
      " - 34s - loss: 0.1260 - acc: 0.8206 - val_loss: 0.1171 - val_acc: 0.8322\n",
      "Epoch 4/10\n",
      " - 35s - loss: 0.1194 - acc: 0.8306 - val_loss: 0.1146 - val_acc: 0.8366\n",
      "Epoch 5/10\n",
      " - 35s - loss: 0.1147 - acc: 0.8384 - val_loss: 0.1135 - val_acc: 0.8362\n",
      "Epoch 6/10\n",
      " - 34s - loss: 0.1108 - acc: 0.8446 - val_loss: 0.1082 - val_acc: 0.8481\n",
      "Epoch 7/10\n",
      " - 34s - loss: 0.1084 - acc: 0.8476 - val_loss: 0.1084 - val_acc: 0.8470\n",
      "Epoch 8/10\n",
      " - 35s - loss: 0.1034 - acc: 0.8584 - val_loss: 0.1179 - val_acc: 0.8305\n",
      "Epoch 9/10\n",
      " - 34s - loss: 0.1017 - acc: 0.8590 - val_loss: 0.1030 - val_acc: 0.8564\n",
      "Epoch 10/10\n",
      " - 34s - loss: 0.0992 - acc: 0.8641 - val_loss: 0.1025 - val_acc: 0.8569\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad,y_train,batch_size=64,epochs=10,validation_data= (X_test_pad,y_test),verbose=2)\n",
    "scores = model.evaluate(X_test_pad, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# printing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.69%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9669348],\n",
       "       [0.6705882],\n",
       "       [0.1296682]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample1=\"just loving it\"\n",
    "test_sample2=\"no comments\"\n",
    "test_sample3=\"totally bad\"\n",
    "\n",
    "\n",
    "test_samples = [test_sample1,test_sample2,test_sample3]\n",
    "test_samples_tokens = tk.texts_to_sequences(test_samples)\n",
    "\n",
    "pad =pad_sequences(test_samples_tokens,maxlen=100)\n",
    "\n",
    "model.predict(x =pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
